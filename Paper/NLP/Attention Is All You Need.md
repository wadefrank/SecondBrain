
Paper: [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)

GitHub(PyTorch): [https://github.com/hyunwoongko/transformer](https://github.com/hyunwoongko/transformer)

# Key Point

the Transformer, based solely on attention mechanisms

- superior in quality
- being more parallelizable
- requiring significantly less time to train

# New Words

dominant

transduction

recurrent

dispensing with

BLEU(bilingual evaluation understudy)

ensembles

fraction

constituency parsing

factor

preclude

factorization

integral

compelling

conjunction

eschew

albeit

counteract