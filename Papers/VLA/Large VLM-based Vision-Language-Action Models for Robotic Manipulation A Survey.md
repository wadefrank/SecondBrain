# 摘要

Robotic manipulation, as a **critical frontier** in robotics and embodied AI, demands precise **motor control** and integrated understanding of visual and semantic cues in dynamic environments.

机器人操控，作为机器人学和具生智能的前沿领域，需要在动态环境中实现精确的运动控制，并整合视觉与语义信息的综合理解。

Traditional approaches, grounded in predefined task specifications and rigid control policies, often struggle to scale or generalize in unstructured, novel scenarios.

传统的方法，建立在预定义的任务规范和严格的控制策略之上，通常很难在非结构化新场景中进行扩展和泛化。

In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm.

近年来，基于由海量图文数据集预训练的大规模视觉语言模型（VLMs, Vision-Language Models）所构建的视觉-语言-动作（VLA, Vision-Language Models）模型，已成为一种变革性的技术范式。

By leveraging large VLMs' capabilities in open-world generalization, hierarchical task planning, knowledge-augmented reasoning, and rich multimodal fusion, these models empower robots to interpret high-level instructions, recognize unseen environments and execute complex manipulation tasks.

通过利用大规模视觉语言模型(Large VLMs)在开放世界泛化、层级任务规划、知识增强的推理和富有的多模态融合的能力，

通过利用大型视觉语言模型（VLMs）在开放世界泛化、分层任务规划、知识增强推理和丰富多模态融合方面的能力，这些模型赋能机器人：

- 解析高层级操作指令
- 辨识未知环境特征
- 执行复杂操控任务